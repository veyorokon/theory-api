ref: llm/litellm@1
image:
  oci: ghcr.io/veyorokon/theory-api/llm-litellm@sha256:1ba33c0574be8068f65f384ef0553d96d8073852d8e0bc7b0aa57baace4fb803
build:
  context: .
  dockerfile: apps/core/processors/llm_litellm/Dockerfile
runtime:
  cpu: '1'
  memory_gb: 2
  timeout_s: 60
secrets:
  required:
  - OPENAI_API_KEY
inputs:
  schema:
    type: object
    properties:
      messages:
        type: array
        items:
          type: object
          properties:
            role:
              type: string
              enum:
              - system
              - user
              - assistant
            content:
              oneOf:
              - type: string
              - type: array
                items:
                  type: object
      model:
        type: string
        default: openai/gpt-4o-mini
      temperature:
        type: number
        minimum: 0
        maximum: 2
        default: 0.7
      max_tokens:
        type: integer
        minimum: 1
        default: 1000
    required:
    - messages
outputs:
- path: text/response.txt
  description: LLM response text
  mime: text/plain
- path: metadata.json
  description: Execution metadata
  mime: application/json
