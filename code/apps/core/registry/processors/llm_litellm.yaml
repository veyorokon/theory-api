name: llm/litellm@1
image: { }
build:
  context: apps/core/processors/llm_litellm
  dockerfile: Dockerfile
runtime:
  cpu: "1"
  memory_gb: 2
  timeout_s: 60
adapter:
  modal:
    stub_name: llm_litellm_v1
    function_name: process
secrets:
  required: [OPENAI_API_KEY]
  optional: [LITELLM_API_BASE]
outputs:
  writes:
    - { kind: prefix, path: /artifacts/outputs/text/ }
entrypoint:
  cmd: ["python","/app/processor.py"]
inputs:
  schema:
    type: object
    properties:
      messages:
        type: array
        items:
          type: object
          properties:
            role:
              type: string
              enum: [system, user, assistant]
            content:
              oneOf:
                - type: string
                - type: array
                  items:
                    type: object
      model:
        type: string
        default: openai/gpt-4o-mini
      temperature:
        type: number
        minimum: 0
        maximum: 2
        default: 0.7
      max_tokens:
        type: integer
        minimum: 1
        default: 1000
    required:
      - messages
outputs:
  - path: text/response.txt
    description: LLM response text
    mime: text/plain
  - path: metadata.json
    description: Execution metadata
    mime: application/json