ref: llm/litellm@1
image:
  oci: ghcr.io/veyorokon/theory-api/llm_litellm@sha256:6a08dac69881455e60d4f11d206089b6de0976567b8cdb1f7a2efd4cfe9496a0
build:
  context: .
  dockerfile: apps/core/processors/llm_litellm/Dockerfile
runtime:
  cpu: '1'
  memory_gb: 2
  timeout_s: 60
secrets:
  required:
  - OPENAI_API_KEY
inputs:
  schema:
    type: object
    properties:
      messages:
        type: array
        items:
          type: object
          properties:
            role:
              type: string
              enum:
              - system
              - user
              - assistant
            content:
              oneOf:
              - type: string
              - type: array
                items:
                  type: object
      model:
        type: string
        default: openai/gpt-4o-mini
      temperature:
        type: number
        minimum: 0
        maximum: 2
        default: 0.7
      max_tokens:
        type: integer
        minimum: 1
        default: 1000
    required:
    - messages
outputs:
- path: text/response.txt
  description: LLM response text
  mime: text/plain
- path: metadata.json
  description: Execution metadata
  mime: application/json
