name: llm/litellm@1
image:
  oci: ghcr.io/veyorokon/llm_litellm@sha256:6314df9935cad6a69486f254c820c079702be35d7d11b604b2d0ea066d86d60b
build:
  context: .
  dockerfile: apps/core/processors/llm_litellm/Dockerfile
runtime:
  cpu: "1"
  memory_gb: 2
  timeout_s: 60
secrets:
  required: [OPENAI_API_KEY]
inputs:
  schema:
    type: object
    properties:
      messages:
        type: array
        items:
          type: object
          properties:
            role:
              type: string
              enum: [system, user, assistant]
            content:
              oneOf:
                - type: string
                - type: array
                  items:
                    type: object
      model:
        type: string
        default: openai/gpt-4o-mini
      temperature:
        type: number
        minimum: 0
        maximum: 2
        default: 0.7
      max_tokens:
        type: integer
        minimum: 1
        default: 1000
    required:
      - messages
outputs:
  - path: text/response.txt
    description: LLM response text
    mime: text/plain
  - path: metadata.json
    description: Execution metadata
    mime: application/json
