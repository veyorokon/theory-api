ref: llm/litellm@1
build:
  context: .
  dockerfile: Dockerfile
  port: 8000
image:
  platforms:
    amd64: ghcr.io/owner/repo/llm-litellm@sha256:REPLACE_AMD64
    arm64: ghcr.io/owner/repo/llm-litellm@sha256:REPLACE_ARM64
runtime:
  cpu: '1'
  memory_gb: 2
  timeout_s: 600
  gpu: null
api:
  protocol: ws
  path: /run
  healthz: /healthz
secrets:
  required:
  - OPENAI_API_KEY
inputs:
  $schema: https://json-schema.org/draft-07/schema#
  title: llm/litellm inputs v1
  type: object
  additionalProperties: false
  required:
  - schema
  - params
  properties:
    schema:
      const: v1
    params:
      type: object
      additionalProperties: false
      required:
      - messages
      properties:
        model:
          type: string
        messages:
          type: array
          minItems: 1
          items:
            type: object
            required:
            - role
            - content
            properties:
              role:
                enum:
                - user
                - system
                - assistant
              content:
                type: string
                minLength: 1
outputs:
- path: text/response.txt
  mime: text/plain
  description: LLM response
- path: metadata.json
  mime: application/json
  description: Execution metadata
