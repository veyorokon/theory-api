name: Modal Deploy

on:
  push:
    branches: [dev, staging, main]
  workflow_dispatch:
    inputs:
      environment:
        description: "Modal environment override (dev|staging|main)"
        required: false
        default: ""

jobs:
  deploy:
    name: Deploy to Modal
    runs-on: ubuntu-latest
    # Map git branch to GitHub Environment and Modal env
    environment: ${{ inputs.environment != '' && inputs.environment || (github.ref_name == 'main' && 'prod' || github.ref_name) }}
    permissions:
      contents: read
    env:
      PIP_DISABLE_PIP_VERSION_CHECK: "1"
      PYTHONUNBUFFERED: "1"
      # Resolve Modal env name (dev|staging|main)
      MODAL_ENVIRONMENT: ${{ inputs.environment != '' && inputs.environment || github.ref_name }}
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install -U pip wheel
          pip install -r requirements.txt
          pip install modal pyyaml

      - name: Extract pinned IMAGE_REF from registry
        id: image
        run: |
          set -euo pipefail
          IMAGE_REF=$(python scripts/ci/get_image_ref.py)
          echo "IMAGE_REF=${IMAGE_REF}" >> $GITHUB_OUTPUT

      - name: Deploy Modal App (committed module)
        env:
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
          IMAGE_REF: ${{ steps.image.outputs.IMAGE_REF }}
          PROCESSOR_REF: "llm/litellm@1"
          TOOL_SECRETS: "OPENAI_API_KEY"
        run: |
          set -euo pipefail
          cd code
          modal deploy --env "${MODAL_ENVIRONMENT}" -m modal_app

      - name: Post-deploy smoke (Modal synthetic transaction)
        working-directory: code
        env:
          DJANGO_SETTINGS_MODULE: backend.settings.unittest
          MODAL_ENABLED: "true"
          MODAL_ENVIRONMENT: ${{ inputs.environment != '' && inputs.environment || github.ref_name }}
          MODAL_FUNCTION_NAME: smoke  # Use deterministic smoke function
        run: |
          set -euo pipefail
          echo "ðŸ§ª Running post-deploy smoke test..."
          
          OUT=$(python manage.py run_processor \
            --ref llm/litellm@1 \
            --adapter modal \
            --write-prefix /artifacts/outputs/smoke/{execution_id}/ \
            --inputs-json '{"messages":[{"role":"user","content":"deployment smoke test"}]}' \
            --json)
          
          echo "ðŸ“‹ Smoke test output:"
          echo "$OUT"
          
          echo "âœ… Verifying deployment success..."
          python - <<'PY' <<< "$OUT"
          import json, sys
          
          try:
              payload = json.loads(input())
              
              # Verify success envelope
              assert payload["status"] == "success", f"Expected success, got: {payload}"
              assert "execution_id" in payload, "Missing execution_id"
              assert payload["execution_id"], "Empty execution_id"
              
              # Verify outputs structure
              assert "outputs" in payload, "Missing outputs"
              assert payload["outputs"], "Empty outputs array"
              
              # Verify canonical output files present
              output_names = sorted([o["path"].rsplit("/",1)[-1] for o in payload["outputs"]])
              expected = ["receipt.json", "response.json"]
              assert output_names == expected, f"Expected {expected}, got {output_names}"
              
              # Verify metadata
              assert "meta" in payload, "Missing meta"
              assert "index_path" in payload, "Missing index_path"
              
              print("âœ… Modal deployment verified successfully")
              print(f"âœ… Execution ID: {payload['execution_id']}")
              print(f"âœ… Output files: {output_names}")
              print(f"âœ… Index path: {payload['index_path']}")
              
              # Small mock OpenAI call for additional verification
              print("ðŸ§ª Testing mock OpenAI call...")
              try:
                  import os
                  os.environ["LLM_PROVIDER"] = "mock"
                  from apps.core.processors.llm_litellm.libs.runtime_common.mock_runner import run_llm
                  mock_result = run_llm({"messages": [{"role": "user", "content": "test"}]})
                  assert "response" in mock_result, "Mock OpenAI call failed"
                  print("âœ… Mock OpenAI call successful")
              except Exception as mock_e:
                  print(f"âš ï¸ Mock OpenAI call warning: {mock_e}")
              
          except Exception as e:
              print(f"âŒ Smoke test verification failed: {e}")
              sys.exit(1)
          PY
